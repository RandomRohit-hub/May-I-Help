import asyncio
import sys
import time

# =================================================
# ğŸ”´ CRITICAL WINDOWS FIX (MUST BE FIRST)
# =================================================
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

print("ğŸš€ Script started", flush=True)

from playwright.async_api import async_playwright
from urllib.parse import urljoin, urlparse

# =================================================
# Pages to extract
# =================================================
URLS = [
    "https://www.playmetrics.site/",
    "https://www.playmetrics.site/docs",
    "https://www.playmetrics.site/about",
    "https://www.playmetrics.site/#Home",
]

# =================================================
# Async scraper function
# =================================================
async def scrape_site():
    print("âœ… Entered scrape_site()", flush=True)

    all_links = set()
    all_text_blocks = []

    print("ğŸ§  Launching Playwright...", flush=True)

    async with async_playwright() as p:
        print("ğŸŒ Launching Chromium browser...", flush=True)

        browser = await p.chromium.launch(
            headless=False,   # ğŸ‘ˆ IMPORTANT: show browser
            slow_mo=50        # ğŸ‘ˆ makes actions visible
        )

        page = await browser.new_page()

        for url in URLS:
            print(f"\nâ¡ï¸ Visiting: {url}", flush=True)

            await page.goto(url, timeout=60000)
            await page.wait_for_timeout(4000)
            await page.wait_for_load_state("networkidle")

            print("ğŸ“„ Extracting text...", flush=True)
            text = await page.inner_text("body")

            all_text_blocks.append(
                f"""
==============================
SOURCE URL:
{url}
==============================

{text}
"""
            )

            print("ğŸ”— Extracting links...", flush=True)
            anchors = await page.query_selector_all("a")

            for a in anchors:
                href = await a.get_attribute("href")
                if not href:
                    continue

                full_url = urljoin(url, href)
                parsed = urlparse(full_url)

                if parsed.scheme in ["http", "https"]:
                    clean_url = full_url.split("#")[0]
                    all_links.add(clean_url)

        print("ğŸ›‘ Closing browser...", flush=True)
        await browser.close()

    # =================================================
    # Save files
    # =================================================
    print("ğŸ’¾ Writing output files...", flush=True)

    joined_text = "\n\n".join(all_text_blocks)
    joined_links = "\n".join(sorted(all_links))

    with open("playmetrics_full_dataset.txt", "w", encoding="utf-8") as f:
        f.write("PLAYMETRICS WEBSITE â€“ EXTRACTED DATASET\n")
        f.write("=====================================\n\n")
        f.write(joined_text)
        f.write("\n\nLINKS\n-----\n")
        f.write(joined_links)

    with open("playmetrics_text_only.txt", "w", encoding="utf-8") as f:
        f.write(joined_text)

    with open("playmetrics_links_only.txt", "w", encoding="utf-8") as f:
        f.write(joined_links)

    print("\nâœ… Extraction completed successfully", flush=True)
    print(f"ğŸ“„ Pages processed: {len(URLS)}", flush=True)
    print(f"ğŸ”— Unique links found: {len(all_links)}", flush=True)
    print("ğŸ“ Files created successfully", flush=True)


# =================================================
# Entry point
# =================================================
if __name__ == "__main__":
    print("â–¶ï¸ Running asyncio loop...", flush=True)
    asyncio.run(scrape_site())
    print("ğŸ Script finished", flush=True)
